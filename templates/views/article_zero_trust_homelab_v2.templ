package views

import (
    "fmt"
	"github.com/ip812/blog/articles"
	"github.com/ip812/blog/templates"
	"github.com/ip812/blog/templates/components"
	"github.com/ip812/blog/templates/button"
	"github.com/ip812/blog/templates/code"
)

var zeroTrustHomelabV2Metadata = articles.GetByID(articles.ZeroTrustHomelabV2ID)

templ ArticleZeroTrustHomelabV2() {
	@templates.Base() {
		<div class="flex flex-col min-h-screen justify-between w-full">
			<div class="flex flex-1 justify-center">
				<div class="mx-auto w-4/5 md:w-1/2 space-y-8 py-12 font-medium">
					<div class="mb-8 flex justify-center">
						@button.Button(button.Props{
							Href: "/p/public/landing-page",
						}) {
                            Go Back Home
						}
					</div>

					<header class="flex flex-col w-full justify-center items-center mb-2">
						<h1 class="text-3xl font-semibold leading-tight">{ zeroTrustHomelabV2Metadata.Name }</h1>
					</header>

                    <h2 id="introduction" class="text-2xl font-bold mt-8 mb-4 group flex items-center">
                        Motivation 
                        <a href="#introduction" class="ml-2 text-blue-600 hover:text-blue-800 text-lg cursor-pointer" aria-label="Link to Introduction">
                            #
                        </a>
                    </h2>

                    <p class="mb-4">
                        In my previous 
                            <a href="/p/public/articles/1417231583613554688" class="text-blue-600 hover:text-blue-800 underline">blog post</a> 
                        I discussed my homeloab setup and how I achived relative HA and more importantly zero opened ports to the internet, by still being able to access my services from anywhere securly
                        and expose my app to the internet. Let me briefly summarize the setup:
                    </p>
                    <ul class="list-disc list-inside mt-0 mb-0">
                        <li>Single EC2 instance in an auto-scaling group, so if the instance crashes, it will be recreated in other AZ.</li>
                        <li>Security group that blocks all ingress and enable all egress.</li>
                        <li>Small k3s cluster running to the instace so to run my applications.</li>
                        <li>The instance was added to my Tailscale network, so easily and securly to be able to access the instace without exposing any ports.</li>
                        <li>Run a cloudflared on the k3s cluster so to expose my web apps via Cloudflare Tunnel to the open internet again by exposing 0 ports.</li>
                        <li>A CloudnativePG was setup so to be able to run my own Postgres databases on the cluster with daily backups and in case of recreation the VM to restore from the backups.(RDS is kinda expensive)</li>
                        <li>Everything is monitored by using Grafana Cloud integration witj Kubernetes, where I store the metrics inside managed Promethes, logs in Loki and so on</li>
                    </ul>
                    <p class="mt-0 mb-0">
                       This setup was and still is great, but I faced some pain points, that really made me think of a some changes:
                    </p>
                    <ul class="list-disc list-inside mt-0 mb-0">
                        <li>
                            I used to manage both my resources and kubernetes setup via Terraform. 
                            While Terraform is great for managing AWS resources, Grafana Cloud stacks and so on, it is kinda painful to manage kubernetes mantifests, helm charts, etc. 
                            How I managed it that I had 2 separate Terraform workspaces, one for the AWS resouece, Cloudflare DNS records, GitHub secrets and so on, while in the second workspaces I picked as outputs the needed results from the first workspace and i created the needed helm charts and so on.
                            This caused one big issue - because my k3s cluster had no export ports and the only way to acess it was to be part of its Tailscale network, I had to make 2 things: 
                            First, I had to the disable the option of Terraform Cloud to run the plans and applies on their workers and to run the plans and applies locally, because there is no way to add their workers to my Tailscale network.
                            So the end setup was to temorarly add Github Action runner temporarly to my Tailscae newtwork(there is a nice Github Action for that) and then run the Terraform plan and apply from that runner.
                        </li>
                        <li>
                            Even that it was possible to manage the k3s cluster via Terraform, it was not ideal.
                            Everytime the VM was recreated I had manually to run the Terraform apply on the k3s workspace, because the state was lost.
                            Other option was (that i did in the majrity of time) was to make a change in the first workspace and then it will trigger a change in the second workspace.
                            As you see this setup was not very robust at least. Also there are some known issues of manageging helm charts via Terraform, especially in case for updating exiting chart or triggering a redeploy.
                        </li>
                    </ul>
                    <p class="mt-0 mb-0">
                       All these points led me to think how to adjust the setup and make it more robust and easier to manage. 
                    </p>

                    <h2 id="introduction" class="text-2xl font-bold mt-8 mb-4 group flex items-center">
                        New Setup 
                        <a href="#introduction" class="ml-2 text-blue-600 hover:text-blue-800 text-lg cursor-pointer" aria-label="Link to Introduction">
                            #
                        </a>
                    </h2>
                    <p class="mt-0 mb-0">
                       At the beginnig I had in mind to use FluxCD to manage the cluster, which has some obvious advantages.
                       Very popular GitOps tool(I though to use ArgoCD at the bigining, but it requres too much resources for my small instance), that can easily manage the cluster via git repos.
                       It is designed to manage kuberentes resources and Helm charts, so it is ideal for my usecase. 
                       Even if i keep the values.yaml files in git, I can easily reuse some command helm charts and make my setup very clean.
                       But there were 2 problems - in these values files how easily to pass information from Terraform and how to pass from terraform all sensitive values like database passwords, cloudflare tunnel credentials and so on from Terraform to the cluster.
                       I put all my secrets in Terraform Cloud, from whgere I can eaily pass them as a senistive variables to a differect terrafomr resoueces.
                       Also somehting that I really like is after I create some resource and I can access some auto generated values from it like DNS names, ARNs and so on.
                       I looked for efficeint way how to pass such non-sensitive values to the values.yaml files, from where FluxCD will pick them up, but I did not find any good solution.
                       That's why I wrote some small terraform provider that will keep in track values file in a git repo and will update them with the values from Terraform.
                    </p>

					<div class="my-0">
						@code.Code(code.Props{
							Language:       "go",
							ShowCopyButton: true,
							Size:           code.SizeFull,
						}) {
							{ 
`resource "gitsync_values_yaml" "go-template" {
  branch  = "main"
  path    = "values/${local.go_template_app_name}.yaml"
  content = <<EOT
isInit: false
name: "${local.go_template_app_name}"
image: "ghcr.io/iypetrov/go-template:1.15.0"
hostname: "${cloudflare_dns_record.go_template_dns_record.name}"
replicas: 1
minMemory: "64Mi"
maxMemory: "128Mi"
minCPU: "50m"
maxCPU: "100m"
healthCheckEndpoint: "/healthz"
env:
  - name: APP_ENV
    value: "${local.env}"
  - name: APP_DOMAIN
    value: "${cloudflare_dns_record.go_template_dns_record.name}"
  - name: APP_PORT
    value: "8080"
  - name: DB_NAME
    value: "${local.go_template_db_name}"
  - name: DB_USERNAME
    valueFrom:
      secretKeyRef:
        name: "${local.go_template_app_name}-creds"
        key: PG_USERNAME
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: "${local.go_template_app_name}-creds"
        key: PG_PASSWORD
  - name: DB_ENDPOINT
    value: "${local.go_template_db_name}-pg-rw.${local.go_template_app_name}.svc.cluster.local"
  - name: DB_SSL_MODE
    value: disable
database:
  postgres:
    name: "${local.go_template_db_name}"
    host: "${local.go_template_db_name}-pg-rw.${local.go_template_app_name}.svc.cluster.local"
    image: "ghcr.io/cloudnative-pg/postgresql:16.1"
    username: "${var.pg_username}"
    storageSize: "1Gi"
    retentionPolicy: "7d"
    backupsBucket: "${aws_s3_bucket.pg_backups.bucket}"
    backupSchedule: "0 0 0 * * *"
EOT
}` }
                        }
                    </div>
                    <p class="mt-0 mb-0">
                       As you can see the senstive values I don't provide them in the values.yaml file, but I reference from which kubernetes secret to pick them up.
                       Okay, but how to pass the sensitive values from Terraform to the cluster as secrets?
                       Before I made some research and used HashiCorp Vault Secrets, but they deprecated it and now for its alternative which is called HashiCorp Vault Dedicated they want at the time of writting ~450$/month, which is crazy for hobby projects
                       Thats why I decided to pick Doppler - it has a free plan, nice intergration with Terraform and Kubernetes.
                       Creation of a secrets looks like this:
                    </p>
					<div class="my-0">
						@code.Code(code.Props{
							Language:       "go",
							ShowCopyButton: true,
							Size:           code.SizeFull,
						}) {
							{ 
`resource "doppler_secret" "pg_password" {
  project = "prod"
  config  = "prd"
  name    = "PG_PASSWORD"
  value   = var.pg_password
}` }
                        }
                    </div>

                    <p class="mt-0 mb-0">
                       Like this I can pick the secret from Doppler and generate a kubernetes secret from it(even there is option with processes to chnage the key value of the secret, if needed): 
                    </p>
					<div class="my-0">
						@code.Code(code.Props{
							Language:       "yaml",
							ShowCopyButton: true,
							Size:           code.SizeFull,
						}) {
							{ 
`---
apiVersion: secrets.doppler.com/v1alpha1
kind: DopplerSecret
metadata:
  name: ghcr-auth-blog
  namespace: doppler-operator-system
spec:
  tokenSecret: 
    name: doppler-token-secret
  project: prod
  config: prd
  managedSecret:
    name: ghcr-auth
    namespace: blog
    type: kubernetes.io/dockerconfigjson
  processors:
    GHCR_DOCKERCONFIGJSON:
      type: plain
      asName: .dockerconfigjson` }
                        }
                    </div>

					<div class="mt-12">
						<h2 class="text-2xl font-bold mb-4">Comments</h2>
						<hr class="border-t-2 border-gray-300 mb-6"/>
                        @components.CommentInputForm(components.CommentInputFormProps{
                            ArticleID: articles.PlaceholderID,
                        })
                        <div 
                            id="comments"
                            hx-get={ fmt.Sprintf("/api/public/v0/articles/%d/comments", articles.PlaceholderID) }
		                    hx-target="#comments"
		                    hx-swap="innerHTML"
                            hx-trigger="load"
                        >
                            <div class="mt-4">
                                @templates.Spinner() {}
                            </div>
                        </div>
					</div>
				</div>
			</div>
			@templates.Footer()
		</div>
	}
}
