package views

import (
    "fmt"
	"github.com/ip812/blog/articles"
	"github.com/ip812/blog/templates"
	"github.com/ip812/blog/templates/components"
	"github.com/ip812/blog/templates/button"
	"github.com/ip812/blog/templates/code"
)

var zeroTrustHomelabV2Metadata = articles.GetByID(articles.ZeroTrustHomelabV2ID)

templ ArticleZeroTrustHomelabV2() {
	@templates.Base() {
		<div class="flex flex-col min-h-screen justify-between w-full">
			<div class="flex flex-1 justify-center">
				<div class="mx-auto w-4/5 md:w-1/2 space-y-8 py-12 font-medium">
					<div class="mb-8 flex justify-center">
						@button.Button(button.Props{
							Href: "/p/public/articles",
						}) {
                            Go Back to Articles
						}
					</div>

					<header class="flex flex-col w-full justify-center items-center mb-2">
						<h1 class="text-3xl font-semibold leading-tight">{ zeroTrustHomelabV2Metadata.Name }</h1>
					</header>

                    <h2 id="motivation" class="text-2xl font-bold mt-8 mb-4 group">
                        <a href="#motivation" class="text-gray-900 hover:text-blue-600 cursor-pointer" aria-label="Link to Motivation">
                            Motivation
                        </a>
                    </h2>

                    <p class="mb-4">
                        In my previous 
                            <a href="/p/public/articles/1417231583613554688" class="text-blue-600 hover:text-blue-800 underline">blog post</a> 
                        I discussed my homelab setup and how I achieved relative HA and more importantly zero opened ports to the internet, while still being able to access my services from anywhere securely
                        and expose my app to the internet. Let me briefly summarize the setup:
                    </p>
                    <ul class="list-disc list-inside mt-0 mb-0">
                        <li>Single EC2 instance in an auto-scaling group, so if the instance crashes, it will be recreated in another AZ.</li>
                        <li>Security group that blocks all ingress and enables all egress.</li>
                        <li>Small k3s cluster running on the instance to run my applications.</li>
                        <li>The instance was added to my Tailscale network, so I can easily and securely access the instance without exposing any ports.</li>
                        <li>Run cloudflared on the k3s cluster to expose my web apps via Cloudflare Tunnel to the open internet again by exposing 0 ports.</li>
                        <li>A CloudnativePG was set up to be able to run my own Postgres databases on the cluster with daily backups and in case of recreation the VM to restore from the backups (RDS is kinda expensive).</li>
                        <li>Everything is monitored by using Grafana Cloud integration with Kubernetes, where I store the metrics inside managed Prometheus, logs in Loki and so on.</li>
                    </ul>
                    <p class="mt-0 mb-0">
                       This setup was and still is great, but I faced some pain points that really made me think of some changes:
                    </p>
                    <ul class="list-disc list-inside mt-0 mb-0">
                        <li>
                            I used to manage both my resources and Kubernetes setup via Terraform. 
                            While Terraform is great for managing AWS resources, Grafana Cloud stacks and so on, it is kinda painful to manage Kubernetes manifests, Helm charts, etc. 
                            How I managed it was that I had 2 separate Terraform workspaces, one for the AWS resources, Cloudflare DNS records, GitHub secrets and so on, while in the second workspace I picked as outputs the needed results from the first workspace and created the needed Helm charts and so on.
                            This caused one big issue - because my k3s cluster had no exposed ports and the only way to access it was to be part of its Tailscale network, I had to do 2 things: 
                            First, I had to disable the option of Terraform Cloud to run the plans and applies on their workers and to run the plans and applies locally, because there is no way to add their workers to my Tailscale network.
                            So the end setup was to temporarily add GitHub Action runner temporarily to my Tailscale network (there is a nice GitHub Action for that) and then run the Terraform plan and apply from that runner.
                        </li>
                        <li>
                            Even though it was possible to manage the k3s cluster via Terraform, it was not ideal.
                            Every time the VM was recreated I had to manually run the Terraform apply on the k3s workspace, because the state was lost.
                            The other option was (that I did in the majority of time) to make a change in the first workspace and then it would trigger a change in the second workspace.
                            As you can see this setup was not very robust at least. Also there are some known issues of managing Helm charts via Terraform, especially in case of updating existing charts or triggering a redeploy.
                        </li>
                    </ul>
                    <p class="mt-0 mb-0">
                       All these points led me to think how to adjust the setup and make it more robust and easier to manage. 
                    </p>

                    <h2 id="new-setup" class="text-2xl font-bold mt-8 mb-4 group">
                        <a href="#new-setup" class="text-gray-900 hover:text-blue-600 cursor-pointer" aria-label="Link to New Setup">
                            New Setup
                        </a>
                    </h2>
                    <p class="mt-0 mb-0">
                       At the beginning I had in mind to use FluxCD to manage the cluster, which has some obvious advantages.
                       Very popular GitOps tool (I thought to use ArgoCD at the beginning, but it requires too much resources for my small instance), that can easily manage the cluster via git repos.
                       It is designed to manage Kubernetes resources and Helm charts, so it is ideal for my use case. 
                       Even if I keep the values.yaml files in git, I can easily reuse some common Helm charts and make my setup very clean.
                       But there were 2 problems - in these values files how to easily pass information from Terraform and how to pass from Terraform all sensitive values like database passwords, Cloudflare tunnel credentials and so on from Terraform to the cluster.
                       I put all my secrets in Terraform Cloud, from where I can easily pass them as sensitive variables to different Terraform resources.
                       Also something that I really like is after I create some resource and I can access some auto-generated values from it like DNS names, ARNs and so on.
                       I looked for an efficient way how to pass such non-sensitive values to the values.yaml files, from where FluxCD will pick them up, but I did not find any good solution.
                       That's why I wrote some small Terraform provider that will keep track of values files in a git repo and will update them with the values from Terraform.
                    </p>

					<div class="my-0">
						@code.Code(code.Props{
							Language:       "go",
							ShowCopyButton: true,
							Size:           code.SizeFull,
						}) {
							{ 
`resource "gitsync_values_yaml" "go-template" {
  branch  = "main"
  path    = "values/${local.go_template_app_name}.yaml"
  content = <<EOT
isInit: false
name: "${local.go_template_app_name}"
image: "ghcr.io/iypetrov/go-template:1.15.0"
hostname: "${cloudflare_dns_record.go_template_dns_record.name}"
replicas: 1
minMemory: "64Mi"
maxMemory: "128Mi"
minCPU: "50m"
maxCPU: "100m"
healthCheckEndpoint: "/healthz"
env:
  - name: APP_ENV
    value: "${local.env}"
  - name: APP_DOMAIN
    value: "${cloudflare_dns_record.go_template_dns_record.name}"
  - name: APP_PORT
    value: "8080"
  - name: DB_NAME
    value: "${local.go_template_db_name}"
  - name: DB_USERNAME
    valueFrom:
      secretKeyRef:
        name: "${local.go_template_app_name}-creds"
        key: PG_USERNAME
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: "${local.go_template_app_name}-creds"
        key: PG_PASSWORD
  - name: DB_ENDPOINT
    value: "${local.go_template_db_name}-pg-rw.${local.go_template_app_name}.svc.cluster.local"
  - name: DB_SSL_MODE
    value: disable
database:
  postgres:
    name: "${local.go_template_db_name}"
    host: "${local.go_template_db_name}-pg-rw.${local.go_template_app_name}.svc.cluster.local"
    image: "ghcr.io/cloudnative-pg/postgresql:16.1"
    username: "${var.pg_username}"
    storageSize: "1Gi"
    retentionPolicy: "7d"
    backupsBucket: "${aws_s3_bucket.pg_backups.bucket}"
    backupSchedule: "0 0 0 * * *"
EOT
}` }
                        }
                    </div>
                    <p class="mt-0 mb-0">
                       As you can see the sensitive values I don't provide them in the values.yaml file, but I reference from which Kubernetes secret to pick them up.
                       Okay, but how to pass the sensitive values from Terraform to the cluster as secrets?
                       Before I made some research and used HashiCorp Vault Secrets, but they deprecated it and now for its alternative which is called HashiCorp Vault Dedicated they want at the time of writing ~450$/month, which is crazy for hobby projects.
                       That's why I decided to pick Doppler - it has a free plan, nice integration with Terraform and Kubernetes.
                       Creation of a secret looks like this:
                    </p>
					<div class="my-0">
						@code.Code(code.Props{
							Language:       "go",
							ShowCopyButton: true,
							Size:           code.SizeFull,
						}) {
							{ 
`resource "doppler_secret" "pg_password" {
  project = "prod"
  config  = "prd"
  name    = "PG_PASSWORD"
  value   = var.pg_password
}` }
                        }
                    </div>

                    <p class="mt-0 mb-0">
                       Like this I can pick the secret from Doppler and generate a Kubernetes secret from it (even there is option with processors to change the key value of the secret, if needed): 
                    </p>
					<div class="my-0">
						@code.Code(code.Props{
							Language:       "yaml",
							ShowCopyButton: true,
							Size:           code.SizeFull,
						}) {
							{ 
`---
apiVersion: secrets.doppler.com/v1alpha1
kind: DopplerSecret
metadata:
  name: ghcr-auth-go-template
  namespace: doppler-operator-system
spec:
  tokenSecret: 
    name: doppler-token-secret
  project: prod
  config: prd
  managedSecret:
    name: ghcr-auth
    namespace: go-template
    type: kubernetes.io/dockerconfigjson
  processors:
    GHCR_DOCKERCONFIGJSON:
      type: plain
      asName: .dockerconfigjson` }
                        }
                    </div>
                    <p class="mt-4 mb-0">
                       With such setup I can easily make a good setup between infra repo that holds the Terraform code for my infrastructure, whenever I want to pass some values from Terraform to the K8s cluster, I just use the gitsync provider and put the needed value in some values.yaml file, where FluxCD will pick it up and will reconcile the cluster.
                    </p>

                    <p class="mt-4 mb-0">
                        So this is I believe the final version of my homelab setup, at least from GitOps perspective.
                        I believe the synergy of Terraform and FluxCD is really powerful and can be used not only for homelabs, but also for production setups.
                        If you have any questions or suggestions, feel free to connect with me on social media or to write a comment below.
                        You can find the source code for this infra repository 
                        <a href="https://github.com/ip812/infra/tree/d4e2cffc171350b2e3e5f9297e9714674229cf02" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer"> 
                            here
                        </a>
                        and for the apps repository
                        <a href="https://github.com/ip812/apps/tree/1c94cb8e71238c501ca5a29f6342b2cf2924de97" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer"> 
                            here
                        </a>.
                    </p>

					<div class="mt-12">
						<h2 class="text-2xl font-bold mb-4">Comments</h2>
						<hr class="border-t-2 border-gray-300 mb-6"/>
                        @components.CommentInputForm(components.CommentInputFormProps{
                            ArticleID: articles.ZeroTrustHomelabV2ID,
                        })
                        <div 
                            id="comments"
                            hx-get={ fmt.Sprintf("/api/public/v0/articles/%d/comments", articles.ZeroTrustHomelabV2ID) }
		                    hx-target="#comments"
		                    hx-swap="innerHTML"
                            hx-trigger="load"
                        >
                            <div class="mt-4">
                                @templates.Spinner() {}
                            </div>
                        </div>
					</div>
				</div>
			</div>
			@templates.Footer()
		</div>
	}
}
